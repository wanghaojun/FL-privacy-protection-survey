### Learning Differentially Private Recurrent Language Models

我们证明，可以用用户级别的差异性隐私保证来训练大型递归语言模型，而预测准确性只需很少的成本。我们的工作基于在用户分区的数据上和用于随机梯度下降的隐私权进行深度网络训练的最新进展。特别是，我们在联合平均算法中添加了用户级别的隐私保护，从而从用户级别的数据中进行了“大步”更新。我们的工作表明，给定一个具有足够多用户的数据集（即使是很小的Internet规模的数据集也很容易满足这一要求），实现差异隐私是以增加计算为代价的，而不是像大多数以前的工作那样以减少效用为代价。我们发现，在大型数据集上进行训练时，我们的私有LSTM语言模型在数量和质量上都与无噪声模型相似。

##### 介绍

LSTM和RNN网络

训练有素的模型可以保护个人数据的隐私，而不会在模型质量上造成不适当的牺牲。

贡献：

我们使用用户相邻数据集的概念将差异隐私应用于模型训练，从而正式保证了用户级别的隐私，而不是单个示例的隐私。

我们在§2中引入了联邦平均算法的杂音版本（McMahan等，2016），该算法通过使用moment accountant（ Abadi等，2016a）来满足用户相邻的差异隐私，moment accountant是为了分析样本级别的差分隐私随机梯度下降而提出的。联合平均方法将多个SGD更新分组在一起，从而实现大步模型更新。

我们在§3中演示了第一个高质量的LSTM语言模型，该模型经过了严格的隐私保证，经过训练，在数据量足够大的情况下，模型的准确性也没有明显降低。例如，在763,430个用户的数据集上，基线（非私有）培训在4120轮培训中达到了17.5％的准确度，其中每轮我们使用100位随机用户的数据。我们在4980个回合中通过（4.6,10−9）差分隐私实现了相同的精度水平，平均每回合处理5000个用户-以大约60倍的可观计算成本维持相同的精度水平。1在具有108个用户的较大数据集上，隐私保证将提高为（1.2,10-9）。通过使用联合平均算法，尽管LSTM具有复杂的内部结构（带有每个单词的嵌入以及密集的状态转换），但我们仍保证了隐私并保持实用性。我们证明，受噪音干扰的模型的指标和定性行为（相对于用词而言）与非私有模型没有显着差异。据我们所知，我们的工作代表了最复杂的机器学习模型，该模型是根据模型的大小和复杂性来判断的，是有隐私保证训练的，也是第一个由用户级别的隐私训练的模型。

在第3节中的大量实验中，我们提供了在训练具有差分隐私保证的复杂模型时进行参数调整的指南。我们表明，少量实验可以将参数空间缩小为一种体制，在这种体制下，我们为隐私付出的代价不是效用的损失，而是计算成本的增加。

背景知识：

差分隐私定义

相邻用户数据集：d和d’是两个训练样本的数据集，每个样本都与一个用户相关。那么，如果可以通过从d中添加或者删除与单个用户相关的所有样本来形成d‘，那么d和d’相邻。

隐私保护目的：查看模型训练的对手无法推断训练中是否使用了任何特定用户的数据，无论他们可能拥有什么辅助信息。

##### 用户级别的差分隐私训练算法

参考工作：FedAvg和moments accountant

加权平均查询的有界灵敏度估计量

多层模型的裁剪策略

隐私保证

大型数据集的差分隐私

##### 实验结果

模型结构：LSTM变体

数据集：Reddit post

构建DP：采样 估计 裁剪和噪声

估计大型数据集专用模型的准确性

随着训练的进行调整噪音和削波

比较DP和非DP模型

##### 总结

在这项工作中，我们介绍了一种用于大型神经网络的用户级差分私有训练的算法，尤其是用于下一词预测的复杂序列模型。我们根据实际数据集对算法进行了经验评估，并证明了这种训练可以在实用程序损失可忽略不计的情况下进行，而无需支付额外的计算费用。这种私人培训与联合学习（将敏感的培训数据保留在设备上而不是集中在联邦学习中）相结合，显示出可以为重要的现实世界应用提供具有显着隐私保证的培训模型。未来的工作还有很多，例如设计私有算法以自动执行并自适应调整削波/噪声权衡，以及将其应用于更广泛的模型族和体系结构，例如GRU和字符级模型。我们的工作还突出了减少非凸模型的差分私有训练的计算开销的开放方向。

### Differentially Private Federated Learning :A Client Level Perspective

##### 摘要

联合学习是隐私保护方面的最新进展。在这种情况下，受信任的管理者聚合由多个客户端以分散方式优化的参数。然后将生成的模型分发回所有客户端，最终收敛为联合的代表模型，而无需明确共享数据。但是，该协议容易受到差分攻击，这种攻击可能源自在联合优化过程中做出贡献的任何一方。在这种攻击中，将通过分析分布式模型来揭示客户在培训期间的贡献以及有关其数据集的信息。我们解决了这个问题，并提出了一种用于客户端差分隐私保护联合优化的算法。目的是在培训期间隐藏客户的贡献，平衡隐私损失和模型性能之间的权衡。实证研究表明，在有足够数量的参与客户的情况下，我们提出的程序可以在模型性能上以较小的成本维持客户级的差异隐私。

##### 介绍

我们要确保学习的模型不会揭示客户是否参与了分散培训。这意味着一个客户端的整个数据集受到保护，不会受到来自其他客户端的差分攻击。

我们的主要贡献：

首先，我们证明了在联合学习中保持模型性能高的同时，可以隐藏客户的参与。我们证明了我们提出的算法可以在模型性能损失较小的情况下实现客户端级别的差异隐私。同时发表的一项独立研究[6]提出了针对客户端级别-dp的类似程序。但是实验设置不同，[6]还包括元素级隐私措施。

其次，我们建议在分散训练过程中动态调整dp保护机制。实证研究表明，这种方式会提高模型性能。这与具有差异性隐私的集中式培训的最新进展形成了鲜明对比，但这种适应没有益处。我们可以将这种差异与以下事实联系起来：与集中式学习相比，联合学习中的梯度在整个培训过程中对噪声和批量大小表现出不同的敏感性。

##### 背景

###### 联邦学习

联邦平均算法 模型平均

###### 差分隐私学习

差分隐私定义

高斯机制

差分隐私梯度下降

###### 联邦学习中的客户端差分隐私

将一种随机机制纳入联盟学习中。但是，与差分隐私深度学习不同，我们的目的不是保护单个数据点在学习模型中的贡献。相反，我们旨在保护整个客户的数据集。也就是说，我们要确保学习的模型不会在保持高模型表现的同时揭示客户是否参与了分散培训。

##### 方法

在联邦平均算法的框架中，中央策展人在每个通信回合后对客户模型（即权重矩阵）求平均值。在我们提出的算法中，我们将使用随机机制更改和近似此平均值。这样做是为了将单个客户的贡献隐藏在聚合中，从而隐藏在整个分布式学习过程中。

两步：

随机子采样（Random sub-sampling）：在每轮训练时随机选取所有客户端的子集。

扭曲：使用高斯机制来扭曲所有更新的和，并且对更新进行了缩放

##### 实验

为了测试我们提出的算法，我们模拟了联合设置。为了可比性，我们选择与[5]相似的实验设置。我们将排序的MNIST集划分为碎片。因此，每个客户端都有两个分片。这样，大多数客户将只能获得两位数的样本。因此，单个客户永远无法在他们的数据上训练模型，以使其对所有十位数字都达到很高的分类精度。

实验规模为100，1000，10000个客户端，四个变量：每个客户端的batch大小；每个客户端的轮次；每次参与回合的客户端数目；GM参数

##### 结果

##### 讨论

##### 总结

通过第一批实证研究，我们证明了在客户级别进行差异化隐私是可行的，并且当涉及到足够多的参与方时可以达到较高的模型精度。此外，我们表明，仔细研究数据和更新分布可以导致优化的隐私预算。对于未来的工作，我们从信噪比，通信代表性，数据表示性以及客户端之间的方差等方面得出最佳边界，并进一步研究与信息论的联系。

### Differentially Private Learning with Adaptive Clipping

##### 摘要

我们引入了一种新的自适应裁剪技术，用于训练具有用户级别的差分隐私的学习模型，从而无需进行大量参数调整。解决此问题的先前方法使用具有噪声更新的联合随机梯度下降或联合平均算法，以及使用Moments  Accountant计算差异隐私保证。这些方法依赖于为每个用户的模型更新选择一个规范界限，需要对其进行仔细调整。最佳值取决于学习率，模型架构，对每个用户的数据进行的通信次数以及可能的其他各种参数。我们显示，基于未裁剪的规范分布的目标分位数的差分私有估计，自适应地设置应用于每个用户更新的裁剪规范就足以消除对此类广泛参数调整的需求。

##### 介绍

梯度下降

差分隐私

联邦学习

联邦SGD->样本级别DP；联邦平均算法->用户级别差分隐私

###### 相关工作

###### 动机

为样本添加限制对于学习过程和差分隐私都是必要的，当前设置固定的限制的方法有很大的缺陷

限制学习样本在学习过程中的影响是必要的，否则任何一个样本都可能导致过拟合。一种限制样本在学习工程中贡献的方法是限制梯度更新的L2范数，限阶为C，任何大于C的梯度更新的L2范数都被被裁剪到C，然后再发给服务器。这种裁剪还有效地限制了系统相对于从训练集中添加或删除任何示例的敏感性。结果，添加适当的噪声后限幅足以实现系统的差异性隐私保证。

为限幅阈值C设置适当的值对于差分私有学习系统的实用性至关重要，因为将其设置得太低会导致较高的信息丢失，而将其设置得太高则会导致增加很多噪声。两种情况都可以降低学习过程的信噪比，这可能会对学习的模型产生不利影响。可以在先前的工作中观察到这种行为[19]，该工作显示了通过各种C值学习的差分私有语言模型的性能。

使用联合平均/  SGD算法[17、19]学习大型模型可能需要在中央服务器和客户端之间进行数千轮交互。更新的规范可以随回合的进行而变化。结果，即使在整个学习过程中设置恒定的限幅阈值也会导致系统的实用性降低。先前的工作[19]表明，在对语言模型进行了一些初始轮次训练之后，降低裁剪阈值的值实际上会提高系统的准确性。但是，如果没有关于系统的先验知识，则规范的行为可能很难预测，并且进行实验以了解此类行为的效率可能较低。

由于学习系统的每一层都可以提供不同的功能，因此在某些情况下按层裁剪更新（即逐层裁剪[19]）可能很有用。但是，如图1所示，各个层的范数可以具有不同的大小，从而使有效搜索裁剪参数的空间更加困难。因此，需要一种能够“即时”学习的系统，以在确保隐私的同时获得较高的实用性。

##### 差分隐私自适应分位数裁剪

在本节中，我们将描述可用于根据更新范式值调整限幅阈值的自适应策略。首先，我们将描述自适应分位数裁剪策略，该策略是为迭代隐私机制设计的。接下来，我们将描述特定于层的噪声添加策略，以使该实用程序比将相同比例的噪声添加到每一层的基本策略具有更高的实用性。

###### 估计分位数的损失函数





### cpSGD:  Communication-efficient and differentially-private distributed SGD

##### 摘要

分布式随机梯度下降是分布式学习中的重要子程序。当客户端是移动设备时，一个特别令人感兴趣的设置是两个重要的关注点，即通信效率和客户端的隐私。近来的一些工作集中在降低通信成本或引入隐私保证上，但是，已知的所有提议的通信有效方法均未实现隐私保护，已知的隐私机制均未达到通信效率。为此，我们研究了实现通信效率和差异隐私的算法。对于d个变量和n≈d个客户端，该方法使用每个客户端每个坐标的O（log  log（nd））位通信，并确保了恒定的隐私。我们还扩展和改进了对二项式机制的先前分析，结果表明该机制可实现与高斯机制几乎相同的效用，同时所需的表示位数更少，这可以是独立的。

##### 贡献：

二项机制：提出二项机制作为一般机制来发布离散值的数据，之前的研究是1维的数据：现在进行了扩充：

d-维数据：我们将一维二项式机理的分析扩展到d维。与高斯分布不同，二项式不是旋转不变的，这使得分析更加复杂。此分析中使用的关键事实是，二项式分布在均值周围局部旋转不变。

改善：实现了二项机制和高斯机制相同的效果。

##### 内容：

本文的其余部分安排如下。在第2节中，我们回顾了差异隐私的概念，并说明了二项式机制的结果。第3节描述了二项机制。由于SGD的收敛可以减少到每轮梯度估计计算中的误差，因此我们在第4节中正式描述DME问题，并在第5节中陈述结果。在5.2节中，我们提供并分析了在DME中结合量化实施二项式机制。对于每个客户端，主要思想是在发送给服务器之前，将经过适当参数化的二项式分布得出的噪声添加到每个量化值。服务器进一步减去噪声引入的偏差，以实现无偏差的均值估计器。我们在第5.3节中进一步表明，[34]中提出的轮换程序可降低MSE，有助于减少由于差异性隐私而导致的附加错误。

##### 疑问：

没看明白通信量是如何减少的，二项机制为什么可以实现无差估计，既然是无差估计了，为什么还要提随机轮换可以减少误差



### Protection Against Reconstruction and Its Applications in Private Federated Learning

引自  机器之心 

https://www.jiqizhixin.com/articles/2019-09-30-2?from=synced&keyword=%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0

##### 摘要

随着大规模统计学习的发展，数据采集和建模技术逐渐从传统的中心化汇聚及处理改变为在外围设备上本地化处理及建模，例如电话、手表、健身手环等。伴随着分布式存储的数据增长，如何在保证足够数据用于建模的同时保护数据隐私性，这一问题面临着巨大的挑战。本文提出了「本地隐私概念」，即在统计人员或学习者能够观察到数据之前，对这些数据进行差异化、模糊化处理，从而对个人数据提供有效保护。传统的用于本地隐私保护的方法在实际应用中过于严格，特别是在现代高维统计和机器学习问题中往往无法适用。本文基于对手大概率事先能够掌握到的信息有限的考虑，为确保对手无法在一定的误差范围内重建数据，提出了一种有效的数据发布措施，同时设计了一种针对不同隐私要求的统计学习问题的最小最大（minimax）差异性隐私机制。

##### 方法和结果

我们提出并研究了在本地隐私下进行模型拟合的两种方法。由于我们在紧接着的下一部分中讨论的与本地差异性隐私相关的困难，我们重新考虑了本地私有学习中的威胁模型（或披露类型）。我们不考虑可以访问所有数据的对手，而是考虑“好奇的”旁观者，他们希望对个人数据进行解码，但很少有先验信息。对此进行形式化（如我们在第2节中讨论的那样），我们可以考虑对隐私参数ε进行实质上宽松的处理，有时甚至根据问题的规模进行缩放，同时仍提供保护。尽管这使我们脱离了差分隐私的标准保证，但我们仍然可以为我们考虑的围观者类型提供隐私保证。

在分布式模型拟合（联合学习）场景中，这种隐私模型是很自然的。通过提供保护以防止好奇的旁观者，公司可以保护其用户免受内部员工等数据的重构；通过将此更宽松的本地隐私模型封装在更广泛的中央差分隐私层中，我们仍然可以为用户提供令人满意的隐私保证，并保护他们免受强大的外部对手的攻击。

在**第2节**中，我们描述了具有适当隐私保护的好奇对手模型，并演示了（对于这些好奇对手）如何仍然几乎无法准确地重建个人数据。

我们将在**第3节**中详细介绍私有联邦学习系统的原型。

基于上文提到的风险优化目标函数，联邦学习的分布式学习过程（不考虑隐私性）重复如下：

a. 中央服务器的全局参数 theta 广播到一个包含 b 个客户端的子集中，客户端本地样本为 Xi,i=1,...b；

b. 每个客户端计算模型参数更新；

c. 中央服务器汇聚更新的参数形成全局更新参数，并更新全局模型。

在本文提出的考虑隐私安全的架构中，第 2、3 步需要增加额外的安全保护措施：在客户端本地执行的步骤 2 中，应用本地隐私保护机制保护本地数据 Xi（参照上文提到的重建破坏保护）；针对在中央服务器中执行的汇聚步骤 3，采用差异性隐私机制保证模型参数的通信过程是全局私有的。整个反馈环路提供了有效的隐私保护，用户的本地数据不会传输到中央服务器，而集中式隐私保护能够保证过程和最终参数都不存在敏感性披露的风险。

**第4节**，在这个方向上，我们将开发新的（最小最大最优）隐私机制，用于对单位球中的高维向量进行私有化。这些机制比Duchi等人的方案产生了实质性的改进。他们的方案只有在ε≤1的情况下才是最优的，提供了比经典噪声添加方案更大的数量级改进，并且在4.4节，我们提供了一个统一定理，表明基于随机梯度的私人学习方案的渐近行为，结果得出的结论是，对于所有隐私参数ε≤d（d是数据维度）而不是ε∈[0,1]，对于所有统计学习问题，我们都有（最小最大）最优过程。

**第5节**用几个大规模的分布式模型拟合问题验证我们的想法，并且展示了如何在实际程序中进行取舍，我们的过程可以大大改善。我们的方法可以大大改善模型拟合和预测方案；在具有较小隐私参数的本地差异隐私根本无法学习模型的情况下，我们可以实现性能接近非私有方案的模型。

### Scalable and Differentially Private Distributed Aggregation in the Shuffled Model

##### 摘要







### Differentially Private Meta-Learning

##### 摘要

参数传递是元学习的一种众所周知的通用方法，其应用包括小样本学习（few-shot learning），联合学习和强化学习。但是，参数传递算法通常需要共享已对来自特定任务的样本进行过训练的模型，从而使任务所有者容易受到隐私的侵犯。我们在这种情况下进行了首次正式的隐私研究，并将任务全局差异隐私的概念形式化为对更常见的威胁模型的实际放宽。然后，我们针对基于梯度的参数传递提出了一种新的差分私有算法，该算法不仅满足此隐私要求，而且在凸设置中保留了可证明的传递学习保证。从经验上讲，我们将分析应用于具有个性化和小样本分类的联合学习问题，这表明允许从较普遍研究的局部隐私概念放宽到任务全局隐私会导致递归神经语言和图像分类模型的性能大大提高。

##### 介绍

元学习领域为改善机器学习方法的性能和适应性提供了有希望的方向。在较高的水平上，这些方法所利用的关键假设是，从单个学习任务中获得的知识共享可以帮助促进对相似的未见任务的学习。但是，此过程的协作性质（其中必须将特定于任务的信息发送到元学习者并由元学习者使用）也带来了固有的数据隐私风险。

在这项工作中，我们专注于一种流行且灵活的元学习方法，即通过基于梯度的元学习（GBML）进行参数传递。尝试在一系列任务上学习一个通用的初始化模型，这样就可以在新任务上仅需几个梯度步骤就可以学习高性能模型。值得注意的是，随着学习的进行，信息在训练任务和元学习者之间不断流动。为了进行迭代更新，元学习器通过训练特定于任务的模型θ获得当前φ的反馈。

在参数传递方法中，元学习者或任何下游参与者可以潜在地从先前的任务中恢复数据。

与联邦学习的不同之处在于，联邦学习不考虑基于任务的可学习性概念，而是专注于全局联邦学习问题来学习单个全局模型。话虽如此，一个涉及用户个性化的联合设置是一个典型的元学习应用。

贡献：

- 我们是第一个为可能用于元学习的DP的不同概念提供分类法的公司。特别是，我们对一个称为全局任务DP的变体进行形式化，并证明了，它在权衡隐私和准确性方面为常用的学习设置添加了有用的选项。
- 我们提出了第一个DP GBML算法，我们将其构造为满足此隐私设置。此外，我们展示了一个简单的扩展，可用于获取设置的DP组版本以同时保护多个样本。
- 我们的隐私保证总体上成立，而且我们也证明了凸设置中的学习理论结果。我们的学习保证了与任务相似的规模，这是通过特定于任务的最佳参数的接近程度来衡量的
- 我们证明了我们的算法及其理论上的保证，自然可以延续到具有个性化的联合学习中。与之前在DP联合学习作品中考虑的隐私概念相比，据我们所知，我们是第一个同时提供隐私和学习保障的人。
- 从经验上讲，我们证明了我们提出的隐私设置可以在联合语言建模和小样本图像分类任务中实现出色的性能

相关工作：

差分隐私在联邦学习中的应用

安全多方计算

##### 元学习上下文中的隐私

在本节中，我们首先将我们考虑的元学习设置形式化。然后，在介绍可以实现的不同DP概念之前，我们描述GBML设置中出现的各种威胁模型。最后，我们重点介绍我们分析的DP的特定模型和类型。

###### 参数转移元学习

###### GBML的威胁模型

潜在对手：其它任务的所有者或者元学习者（只考虑一个诚实但好奇的元学习者），在这两种情况下，不仅要关注其他参与者的意图，还要关注他们自身的安全性以防止恶意外部人员的访问。

被保护的数据：记录级别和数据集级别（任务级别）鉴于隐私和实用程序经常发生冲突，因此我们经常寻求最弱的隐私概念，以便最好地保留实用程序。我们尝试通过考虑一种从任务级别到记录级别的隐私放松，但保留针对每个任务所有者的所有其他各方的保护的模型，来弥合这些威胁模型之间的鸿沟。

###### 单任务下的差分隐私

全局差分隐私 数据集，保证无法观察数据集中是否含有某条特定记录

本地差分隐私 样本，直接对样本添加噪声，通常会使模型性能变差

###### 对于GBML下的差分隐私

meta-级别：聚合模型过程，将每个任务的模型聚合成一个全局一般模型

within-task级别：任务内训练模型过程

Global DP: 全局模型没有任何关于某个任务模型的细节信息

Local DP: 每个任务模型不会泄露给全局模型

Task-Global DP: 任务模型没有关于某个样本的细节信息

Task-Local DP: 任何样本都不会透露给任务所有者

##### 差分隐私参数传输

算法

隐私保证

学习保证

##### 实验结果

数据集

元学习算法

隐私注意事项

结果

##### 总结

在这项工作中，我们概述并研究了元学习背景下的隐私问题。着眼于基于梯度的参数传递方法的类别，我们通过与中央元学习者共享特定于任务的模型，使用差分隐私解决了任务所有者所面临的隐私风险。为此，我们形式化并考虑了任务全局差异隐私的概念，该概念可确保任务中的单个示例受到所有下游代理（尤其是元学习者）的保护。在此隐私模型下，我们开发了一种差分私有算法，该算法既可以保证这种保护，也可以在凸设置中获得学习理论的结果。最后，我们演示了如何将隐私概念转化为有用的深度学习模型，用于非凸语言建模和图像分类任务。



