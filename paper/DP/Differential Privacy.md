### Learning Differentially Private Recurrent Language Models

我们证明，可以用用户级别的差异性隐私保证来训练大型递归语言模型，而预测准确性只需很少的成本。我们的工作基于在用户分区的数据上和用于随机梯度下降的隐私权进行深度网络训练的最新进展。特别是，我们在联合平均算法中添加了用户级别的隐私保护，从而从用户级别的数据中进行了“大步”更新。我们的工作表明，给定一个具有足够多用户的数据集（即使是很小的Internet规模的数据集也很容易满足这一要求），实现差异隐私是以增加计算为代价的，而不是像大多数以前的工作那样以减少效用为代价。我们发现，在大型数据集上进行训练时，我们的私有LSTM语言模型在数量和质量上都与无噪声模型相似。

##### 介绍

LSTM和RNN网络

训练有素的模型可以保护个人数据的隐私，而不会在模型质量上造成不适当的牺牲。

贡献：

我们使用用户相邻数据集的概念将差异隐私应用于模型训练，从而正式保证了用户级别的隐私，而不是单个示例的隐私。

我们在§2中引入了联邦平均算法的杂音版本（McMahan等，2016），该算法通过使用moment accountant（ Abadi等，2016a）来满足用户相邻的差异隐私，moment accountant是为了分析样本级别的差分隐私随机梯度下降而提出的。联合平均方法将多个SGD更新分组在一起，从而实现大步模型更新。

我们在§3中演示了第一个高质量的LSTM语言模型，该模型经过了严格的隐私保证，经过训练，在数据量足够大的情况下，模型的准确性也没有明显降低。例如，在763,430个用户的数据集上，基线（非私有）培训在4120轮培训中达到了17.5％的准确度，其中每轮我们使用100位随机用户的数据。我们在4980个回合中通过（4.6,10−9）差分隐私实现了相同的精度水平，平均每回合处理5000个用户-以大约60倍的可观计算成本维持相同的精度水平。1在具有108个用户的较大数据集上，隐私保证将提高为（1.2,10-9）。通过使用联合平均算法，尽管LSTM具有复杂的内部结构（带有每个单词的嵌入以及密集的状态转换），但我们仍保证了隐私并保持实用性。我们证明，受噪音干扰的模型的指标和定性行为（相对于用词而言）与非私有模型没有显着差异。据我们所知，我们的工作代表了最复杂的机器学习模型，该模型是根据模型的大小和复杂性来判断的，是有隐私保证训练的，也是第一个由用户级别的隐私训练的模型。

在第3节中的大量实验中，我们提供了在训练具有差分隐私保证的复杂模型时进行参数调整的指南。我们表明，少量实验可以将参数空间缩小为一种体制，在这种体制下，我们为隐私付出的代价不是效用的损失，而是计算成本的增加。

背景知识：

差分隐私定义

相邻用户数据集：d和d’是两个训练样本的数据集，每个样本都与一个用户相关。那么，如果可以通过从d中添加或者删除与单个用户相关的所有样本来形成d‘，那么d和d’相邻。

隐私保护目的：查看模型训练的对手无法推断训练中是否使用了任何特定用户的数据，无论他们可能拥有什么辅助信息。

##### 用户级别的差分隐私训练算法

参考工作：FedAvg和moments accountant

加权平均查询的有界灵敏度估计量

多层模型的裁剪策略

隐私保证

大型数据集的差分隐私

##### 实验结果

模型结构：LSTM变体

数据集：Reddit post

构建DP：采样 估计 裁剪和噪声

估计大型数据集专用模型的准确性

随着训练的进行调整噪音和削波

比较DP和非DP模型

##### 总结

在这项工作中，我们介绍了一种用于大型神经网络的用户级差分私有训练的算法，尤其是用于下一词预测的复杂序列模型。我们根据实际数据集对算法进行了经验评估，并证明了这种训练可以在实用程序损失可忽略不计的情况下进行，而无需支付额外的计算费用。这种私人培训与联合学习（将敏感的培训数据保留在设备上而不是集中在联邦学习中）相结合，显示出可以为重要的现实世界应用提供具有显着隐私保证的培训模型。未来的工作还有很多，例如设计私有算法以自动执行并自适应调整削波/噪声权衡，以及将其应用于更广泛的模型族和体系结构，例如GRU和字符级模型。我们的工作还突出了减少非凸模型的差分私有训练的计算开销的开放方向。

### Differentially Private Federated Learning :A Client Level Perspective

##### 摘要

联合学习是隐私保护方面的最新进展。在这种情况下，受信任的管理者聚合由多个客户端以分散方式优化的参数。然后将生成的模型分发回所有客户端，最终收敛为联合的代表模型，而无需明确共享数据。但是，该协议容易受到差分攻击，这种攻击可能源自在联合优化过程中做出贡献的任何一方。在这种攻击中，将通过分析分布式模型来揭示客户在培训期间的贡献以及有关其数据集的信息。我们解决了这个问题，并提出了一种用于客户端差分隐私保护联合优化的算法。目的是在培训期间隐藏客户的贡献，平衡隐私损失和模型性能之间的权衡。实证研究表明，在有足够数量的参与客户的情况下，我们提出的程序可以在模型性能上以较小的成本维持客户级的差异隐私。

##### 介绍

我们要确保学习的模型不会揭示客户是否参与了分散培训。这意味着一个客户端的整个数据集受到保护，不会受到来自其他客户端的差分攻击。

我们的主要贡献：

首先，我们证明了在联合学习中保持模型性能高的同时，可以隐藏客户的参与。我们证明了我们提出的算法可以在模型性能损失较小的情况下实现客户端级别的差异隐私。同时发表的一项独立研究[6]提出了针对客户端级别-dp的类似程序。但是实验设置不同，[6]还包括元素级隐私措施。

其次，我们建议在分散训练过程中动态调整dp保护机制。实证研究表明，这种方式会提高模型性能。这与具有差异性隐私的集中式培训的最新进展形成了鲜明对比，但这种适应没有益处。我们可以将这种差异与以下事实联系起来：与集中式学习相比，联合学习中的梯度在整个培训过程中对噪声和批量大小表现出不同的敏感性。

##### 背景

###### 联邦学习

联邦平均算法 模型平均

###### 差分隐私学习

差分隐私定义

高斯机制

差分隐私梯度下降

###### 联邦学习中的客户端差分隐私

将一种随机机制纳入联盟学习中。但是，与差分隐私深度学习不同，我们的目的不是保护单个数据点在学习模型中的贡献。相反，我们旨在保护整个客户的数据集。也就是说，我们要确保学习的模型不会在保持高模型表现的同时揭示客户是否参与了分散培训。

##### 方法

在联邦平均算法的框架中，中央策展人在每个通信回合后对客户模型（即权重矩阵）求平均值。在我们提出的算法中，我们将使用随机机制更改和近似此平均值。这样做是为了将单个客户的贡献隐藏在聚合中，从而隐藏在整个分布式学习过程中。

两步：

随机子采样（Random sub-sampling）：在每轮训练时随机选取所有客户端的子集。

扭曲：使用高斯机制来扭曲所有更新的和，并且对更新进行了缩放

##### 实验

为了测试我们提出的算法，我们模拟了联合设置。为了可比性，我们选择与[5]相似的实验设置。我们将排序的MNIST集划分为碎片。因此，每个客户端都有两个分片。这样，大多数客户将只能获得两位数的样本。因此，单个客户永远无法在他们的数据上训练模型，以使其对所有十位数字都达到很高的分类精度。

实验规模为100，1000，10000个客户端，四个变量：每个客户端的batch大小；每个客户端的轮次；每次参与回合的客户端数目；GM参数

##### 结果

##### 讨论

##### 总结

通过第一批实证研究，我们证明了在客户级别进行差异化隐私是可行的，并且当涉及到足够多的参与方时可以达到较高的模型精度。此外，我们表明，仔细研究数据和更新分布可以导致优化的隐私预算。对于未来的工作，我们从信噪比，通信代表性，数据表示性以及客户端之间的方差等方面得出最佳边界，并进一步研究与信息论的联系。

### Differentially Private Learning with Adaptive Clipping

##### 摘要

我们引入了一种新的自适应裁剪技术，用于训练具有用户级别的差分隐私的学习模型，从而无需进行大量参数调整。解决此问题的先前方法使用具有噪声更新的联合随机梯度下降或联合平均算法，以及使用Moments  Accountant计算差异隐私保证。这些方法依赖于为每个用户的模型更新选择一个规范界限，需要对其进行仔细调整。最佳值取决于学习率，模型架构，对每个用户的数据进行的通信次数以及可能的其他各种参数。我们显示，基于未裁剪的规范分布的目标分位数的差分私有估计，自适应地设置应用于每个用户更新的裁剪规范就足以消除对此类广泛参数调整的需求。

##### 介绍

梯度下降

差分隐私

联邦学习

联邦SGD->样本级别DP；联邦平均算法->用户级别差分隐私

###### 相关工作

###### 动机

为样本添加限制对于学习过程和差分隐私都是必要的，当前设置固定的限制的方法有很大的缺陷

限制学习样本在学习过程中的影响是必要的，否则任何一个样本都可能导致过拟合。一种限制样本在学习工程中贡献的方法是限制梯度更新的L2范数，限阶为C，任何大于C的梯度更新的L2范数都被被裁剪到C，然后再发给服务器。这种裁剪还有效地限制了系统相对于从训练集中添加或删除任何示例的敏感性。结果，添加适当的噪声后限幅足以实现系统的差异性隐私保证。

为限幅阈值C设置适当的值对于差分私有学习系统的实用性至关重要，因为将其设置得太低会导致较高的信息丢失，而将其设置得太高则会导致增加很多噪声。两种情况都可以降低学习过程的信噪比，这可能会对学习的模型产生不利影响。可以在先前的工作中观察到这种行为[19]，该工作显示了通过各种C值学习的差分私有语言模型的性能。

使用联合平均/  SGD算法[17、19]学习大型模型可能需要在中央服务器和客户端之间进行数千轮交互。更新的规范可以随回合的进行而变化。结果，即使在整个学习过程中设置恒定的限幅阈值也会导致系统的实用性降低。先前的工作[19]表明，在对语言模型进行了一些初始轮次训练之后，降低裁剪阈值的值实际上会提高系统的准确性。但是，如果没有关于系统的先验知识，则规范的行为可能很难预测，并且进行实验以了解此类行为的效率可能较低。

由于学习系统的每一层都可以提供不同的功能，因此在某些情况下按层裁剪更新（即逐层裁剪[19]）可能很有用。但是，如图1所示，各个层的范数可以具有不同的大小，从而使有效搜索裁剪参数的空间更加困难。因此，需要一种能够“即时”学习的系统，以在确保隐私的同时获得较高的实用性。

##### 差分隐私自适应分位数裁剪

在本节中，我们将描述可用于根据更新范式值调整限幅阈值的自适应策略。首先，我们将描述自适应分位数裁剪策略，该策略是为迭代隐私机制设计的。接下来，我们将描述特定于层的噪声添加策略，以使该实用程序比将相同比例的噪声添加到每一层的基本策略具有更高的实用性。

###### 估计分位数的损失函数





### cpSGD:  Communication-efficient and differentially-private distributed SGD

##### 摘要

分布式随机梯度下降是分布式学习中的重要子程序。当客户端是移动设备时，一个特别令人感兴趣的设置是两个重要的关注点，即通信效率和客户端的隐私。近来的一些工作集中在降低通信成本或引入隐私保证上，但是，已知的所有提议的通信有效方法均未实现隐私保护，已知的隐私机制均未达到通信效率。为此，我们研究了实现通信效率和差异隐私的算法。对于d个变量和n≈d个客户端，该方法使用每个客户端每个坐标的O（log  log（nd））位通信，并确保了恒定的隐私。我们还扩展和改进了对二项式机制的先前分析，结果表明该机制可实现与高斯机制几乎相同的效用，同时所需的表示位数更少，这可以是独立的。

##### 贡献：

二项机制：提出二项机制作为一般机制来发布离散值的数据，之前的研究是1维的数据：现在进行了扩充：

d-维数据：我们将一维二项式机理的分析扩展到d维。与高斯分布不同，二项式不是旋转不变的，这使得分析更加复杂。此分析中使用的关键事实是，二项式分布在均值周围局部旋转不变。

改善：实现了二项机制和高斯机制相同的效果。

##### 内容：

本文的其余部分安排如下。在第2节中，我们回顾了差异隐私的概念，并说明了二项式机制的结果。第3节描述了二项机制。由于SGD的收敛可以减少到每轮梯度估计计算中的误差，因此我们在第4节中正式描述DME问题，并在第5节中陈述结果。在5.2节中，我们提供并分析了在DME中结合量化实施二项式机制。对于每个客户端，主要思想是在发送给服务器之前，将经过适当参数化的二项式分布得出的噪声添加到每个量化值。服务器进一步减去噪声引入的偏差，以实现无偏差的均值估计器。我们在第5.3节中进一步表明，[34]中提出的轮换程序可降低MSE，有助于减少由于差异性隐私而导致的附加错误。

##### 疑问：

没看明白通信量是如何减少的，二项机制为什么可以实现无差估计，既然是无差估计了，为什么还要提随机轮换可以减少误差



### Protection Against Reconstruction and Its Applications in Private Federated Learning

引自  机器之心 

https://www.jiqizhixin.com/articles/2019-09-30-2?from=synced&keyword=%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0

##### 摘要

随着大规模统计学习的发展，数据采集和建模技术逐渐从传统的中心化汇聚及处理改变为在外围设备上本地化处理及建模，例如电话、手表、健身手环等。伴随着分布式存储的数据增长，如何在保证足够数据用于建模的同时保护数据隐私性，这一问题面临着巨大的挑战。本文提出了「本地隐私概念」，即在统计人员或学习者能够观察到数据之前，对这些数据进行差异化、模糊化处理，从而对个人数据提供有效保护。传统的用于本地隐私保护的方法在实际应用中过于严格，特别是在现代高维统计和机器学习问题中往往无法适用。本文基于对手大概率事先能够掌握到的信息有限的考虑，为确保对手无法在一定的误差范围内重建数据，提出了一种有效的数据发布措施，同时设计了一种针对不同隐私要求的统计学习问题的最小最大（minimax）差异性隐私机制。

##### 方法和结果

我们提出并研究了在本地隐私下进行模型拟合的两种方法。由于我们在紧接着的下一部分中讨论的与本地差异性隐私相关的困难，我们重新考虑了本地私有学习中的威胁模型（或披露类型）。我们不考虑可以访问所有数据的对手，而是考虑“好奇的”旁观者，他们希望对个人数据进行解码，但很少有先验信息。对此进行形式化（如我们在第2节中讨论的那样），我们可以考虑对隐私参数ε进行实质上宽松的处理，有时甚至根据问题的规模进行缩放，同时仍提供保护。尽管这使我们脱离了差分隐私的标准保证，但我们仍然可以为我们考虑的围观者类型提供隐私保证。

在分布式模型拟合（联合学习）场景中，这种隐私模型是很自然的。通过提供保护以防止好奇的旁观者，公司可以保护其用户免受内部员工等数据的重构；通过将此更宽松的本地隐私模型封装在更广泛的中央差分隐私层中，我们仍然可以为用户提供令人满意的隐私保证，并保护他们免受强大的外部对手的攻击。

在**第2节**中，我们描述了具有适当隐私保护的好奇对手模型，并演示了（对于这些好奇对手）如何仍然几乎无法准确地重建个人数据。

我们将在**第3节**中详细介绍私有联邦学习系统的原型。

基于上文提到的风险优化目标函数，联邦学习的分布式学习过程（不考虑隐私性）重复如下：

a. 中央服务器的全局参数 theta 广播到一个包含 b 个客户端的子集中，客户端本地样本为 Xi,i=1,...b；

b. 每个客户端计算模型参数更新；

c. 中央服务器汇聚更新的参数形成全局更新参数，并更新全局模型。

在本文提出的考虑隐私安全的架构中，第 2、3 步需要增加额外的安全保护措施：在客户端本地执行的步骤 2 中，应用本地隐私保护机制保护本地数据 Xi（参照上文提到的重建破坏保护）；针对在中央服务器中执行的汇聚步骤 3，采用差异性隐私机制保证模型参数的通信过程是全局私有的。整个反馈环路提供了有效的隐私保护，用户的本地数据不会传输到中央服务器，而集中式隐私保护能够保证过程和最终参数都不存在敏感性披露的风险。

**第4节**，在这个方向上，我们将开发新的（最小最大最优）隐私机制，用于对单位球中的高维向量进行私有化。这些机制比Duchi等人的方案产生了实质性的改进。他们的方案只有在ε≤1的情况下才是最优的，提供了比经典噪声添加方案更大的数量级改进，并且在4.4节，我们提供了一个统一定理，表明基于随机梯度的私人学习方案的渐近行为，结果得出的结论是，对于所有隐私参数ε≤d（d是数据维度）而不是ε∈[0,1]，对于所有统计学习问题，我们都有（最小最大）最优过程。

**第5节**用几个大规模的分布式模型拟合问题验证我们的想法，并且展示了如何在实际程序中进行取舍，我们的过程可以大大改善。我们的方法可以大大改善模型拟合和预测方案；在具有较小隐私参数的本地差异隐私根本无法学习模型的情况下，我们可以实现性能接近非私有方案的模型。

### Differentially Private Meta-Learning

##### 摘要

参数传递是元学习的一种众所周知的通用方法，其应用包括小样本学习（few-shot learning），联合学习和强化学习。但是，参数传递算法通常需要共享已对来自特定任务的样本进行过训练的模型，从而使任务所有者容易受到隐私的侵犯。我们在这种情况下进行了首次正式的隐私研究，并将任务全局差异隐私的概念形式化为对更常见的威胁模型的实际放宽。然后，我们针对基于梯度的参数传递提出了一种新的差分私有算法，该算法不仅满足此隐私要求，而且在凸设置中保留了可证明的传递学习保证。从经验上讲，我们将分析应用于具有个性化和小样本分类的联合学习问题，这表明允许从较普遍研究的局部隐私概念放宽到任务全局隐私会导致递归神经语言和图像分类模型的性能大大提高。

##### 介绍

元学习领域为改善机器学习方法的性能和适应性提供了有希望的方向。在较高的水平上，这些方法所利用的关键假设是，从单个学习任务中获得的知识共享可以帮助促进对相似的未见任务的学习。但是，此过程的协作性质（其中必须将特定于任务的信息发送到元学习者并由元学习者使用）也带来了固有的数据隐私风险。

在这项工作中，我们专注于一种流行且灵活的元学习方法，即通过基于梯度的元学习（GBML）进行参数传递。尝试在一系列任务上学习一个通用的初始化模型，这样就可以在新任务上仅需几个梯度步骤就可以学习高性能模型。值得注意的是，随着学习的进行，信息在训练任务和元学习者之间不断流动。为了进行迭代更新，元学习器通过训练特定于任务的模型θ获得当前φ的反馈。

在参数传递方法中，元学习者或任何下游参与者可以潜在地从先前的任务中恢复数据。

与联邦学习的不同之处在于，联邦学习不考虑基于任务的可学习性概念，而是专注于全局联邦学习问题来学习单个全局模型。话虽如此，一个涉及用户个性化的联合设置是一个典型的元学习应用。

贡献：

- 我们是第一个为可能用于元学习的DP的不同概念提供分类法的公司。特别是，我们对一个称为全局任务DP的变体进行形式化，并证明了，它在权衡隐私和准确性方面为常用的学习设置添加了有用的选项。
- 我们提出了第一个DP GBML算法，我们将其构造为满足此隐私设置。此外，我们展示了一个简单的扩展，可用于获取设置的DP组版本以同时保护多个样本。
- 我们的隐私保证总体上成立，而且我们也证明了凸设置中的学习理论结果。我们的学习保证了与任务相似的规模，这是通过特定于任务的最佳参数的接近程度来衡量的
- 我们证明了我们的算法及其理论上的保证，自然可以延续到具有个性化的联合学习中。与之前在DP联合学习作品中考虑的隐私概念相比，据我们所知，我们是第一个同时提供隐私和学习保障的人。
- 从经验上讲，我们证明了我们提出的隐私设置可以在联合语言建模和小样本图像分类任务中实现出色的性能

相关工作：

差分隐私在联邦学习中的应用

安全多方计算

##### 元学习上下文中的隐私

在本节中，我们首先将我们考虑的元学习设置形式化。然后，在介绍可以实现的不同DP概念之前，我们描述GBML设置中出现的各种威胁模型。最后，我们重点介绍我们分析的DP的特定模型和类型。

###### 参数转移元学习

###### GBML的威胁模型

潜在对手：其它任务的所有者或者元学习者（只考虑一个诚实但好奇的元学习者），在这两种情况下，不仅要关注其他参与者的意图，还要关注他们自身的安全性以防止恶意外部人员的访问。

被保护的数据：记录级别和数据集级别（任务级别）鉴于隐私和实用程序经常发生冲突，因此我们经常寻求最弱的隐私概念，以便最好地保留实用程序。我们尝试通过考虑一种从任务级别到记录级别的隐私放松，但保留针对每个任务所有者的所有其他各方的保护的模型，来弥合这些威胁模型之间的鸿沟。

###### 单任务下的差分隐私

全局差分隐私 数据集，保证无法观察数据集中是否含有某条特定记录

本地差分隐私 样本，直接对样本添加噪声，通常会使模型性能变差

###### 对于GBML下的差分隐私

meta-级别：聚合模型过程，将每个任务的模型聚合成一个全局一般模型

within-task级别：任务内训练模型过程

Global DP: 全局模型没有任何关于某个任务模型的细节信息

Local DP: 每个任务模型不会泄露给全局模型

Task-Global DP: 任务模型没有关于某个样本的细节信息

Task-Local DP: 任何样本都不会透露给任务所有者

##### 差分隐私参数传输

算法

隐私保证

学习保证

##### 实验结果

数据集

元学习算法

隐私注意事项

结果

##### 总结

在这项工作中，我们概述并研究了元学习背景下的隐私问题。着眼于基于梯度的参数传递方法的类别，我们通过与中央元学习者共享特定于任务的模型，使用差分隐私解决了任务所有者所面临的隐私风险。为此，我们形式化并考虑了任务全局差异隐私的概念，该概念可确保任务中的单个示例受到所有下游代理（尤其是元学习者）的保护。在此隐私模型下，我们开发了一种差分私有算法，该算法既可以保证这种保护，也可以在凸设置中获得学习理论的结果。最后，我们演示了如何将隐私概念转化为有用的深度学习模型，用于非凸语言建模和图像分类任务。

### Federated Learning with Bayesian Differential Privacy

##### 摘要

我们考虑通过正式的隐私保证来加强联邦学习的问题。我们建议采用贝叶斯差分隐私，这是对类似分布的数据的差分隐私的放松，以提供更清晰的隐私丢失范围。我们将贝叶斯隐私会计方法调整为适用于联邦设置，并建议进行多项改进以提高不同级别的隐私预算效率。我们的实验表明，对于图像分类任务（包括医疗应用）的联邦学习而言，与最新的差分隐私界限相比，它具有显着优势，这使客户端级别的隐私预算低于ε=  1，在样本级别低于ε=  0.1。较低的噪声量也有助于提高模型的准确性，并减少通信回合的次数。

##### 介绍

如最近的工作[10]（Learning differentially private recurrent language models），（Differentially private federated
learning: A client level perspective）所示，可以将这两种方法结合起来以提供共同的利益。但是，除非用户数量过高（例如，在[10]中考虑的移动用户数量众多的情况下），否则差分私有联合学习只能提供较弱的保证。与机器学习界的广泛观点相反，接近于10的ε值几乎不能视为对用户的放心：对于某些类型的攻击，对手在理论上可以达到99.99％的准确性。

我们建议通过自然放松差异性隐私（Bayesian differential
privacy ,BDP）来增强联邦学习，从而提供更紧密，因此更有意义的保证。这种放松的主要思想是基于这样的观察：机器学习任务通常仅限于特定类型的数据（例如，很难在MRI数据集中找到电影评论）。而且，攻击者通常可以使用此信息，甚至可能还有一些先验数据分布。传统的DP将所有数据均等地对待，并通过大量噪声隐藏差异，而BDP则将噪声校准为数据分布。因此，对于从相同（任意）分布得出的任何两个数据集，并在给定相同的隐私机制且噪声量相同的情况下，BDP比DP提供了更严格的保证。注意，由于可以根据数据估算必要的统计信息，因此不需要完全了解此分布。

第四节介绍贝叶斯差分隐私；并将其扩展到第五节中的联合学习环境。我们的实验（请参见第六节）在隐私保证和模型质量方面都显示出显着优势。

贡献：

- 我们将贝叶斯差分隐私的概念调整为适用于联合学习，包括更自然的no-iid. 设置（第V-A节），以在次要和实际假设下提供强大的理论隐私保证；
- 我们提出了一种新颖的联合计费方法，用于同时安全地估计客户端级别和实例级别的隐私（第V -C节）；
- 我们通过实验证明了我们方法的优势，例如将隐私预算缩减到以前的最新技术水平的一小部分，并将经过训练的模型的准确性提高多达10％（第六部分）。

##### 相关工作

在这项工作中，我们依靠另一种放松，即贝叶斯差分隐私[12]。通过利用数据来自特定分布的事实，并且并非所有数据都具有同等可能性，这一概念提高了隐私会计的效率。同时，它保持其参数ε和δ的概率解释。值得注意的是，与上述某些放松方式不同，贝叶斯DP的概念（在特定条件下）提供最坏情况的保证，并且不限于特定的数据集，而是特定类型的数据（例如电子邮件，  MRI图像等），或这些类型的混合，这是一个更容易接受的假设。

##### 技术

A.定义和概念

定义1.差分隐私

定义2.隐私损失

定义3.高斯机制

定义4.Renyi散度

B.设置

C.动机

##### 贝叶斯差分隐私

![](http://img.wanghaojun.cn//img/20210106183026.png)

##### 贝叶斯差分隐私联邦学习

在本节中，我们采用贝叶斯差分隐私框架及其会计方法来保证客户级别的隐私，这是文献中最常提到的级别。然后，我们证明并探索实例级隐私以及两种不同的计费技术。最后，我们提出了一种为FedSGD算法联合考虑实例级别和客户端级别隐私的方法，以便在效用和隐私保证之间提供最强的权衡。

A.客户端隐私

当要通过差异性隐私加强联合学习时，首先要注意的是客户端级别的隐私[10]，[11]。目标是隐藏单个用户的存在，或更具体地讲，是要限制任何单个用户对学习结果分布（即模型参数的分布）的影响。

使用 Bayesian account而不是moment account，在用户间数据分布是相似的环境下，他们的更新将保持一致，Bayesian account比moment account更具优势。

B.样本隐私

两种隐私计算方式：

Sequential accounting：部分计算在用户设备上本地执行，部分在服务器上执行。

Parallel accounting：

C.共同隐私

实例隐私为每个数据点对训练模型的贡献提供了更严格，更有意义的保证。尽管如此，这还有一个缺点：在设备上的梯度下降过程中以及在服务器的平均阶段都增加噪声会导致联合学习算法收敛缓慢或完全发散。为了解决此问题，我们建议采用联合计费，其中将在客户端增加的噪音重新计入客户端级别的隐私保证。

联合记帐的主要思想是，在强制执行实例隐私时，服务器收到的客户端更新已经很嘈杂，并且服务器可以重新计数现有的噪声以计算客户端级别的界限，而不是添加更多的噪声。唯一的问题：服务器无法采样非私有客户端更新来估计ct（λ），因为它不再有权访问它们的分布。

使用联合记帐可以实现严格的实例和客户端隐私保证，并同时将收敛速度保持在与仅客户端隐私解决方案几乎相同的水平上

##### 实验

##### 总结

我们采用（ε，δ）-贝叶斯差分隐私的概念，即（ε，δ）-差分隐私的松弛，以在联合学习设置中为客户获得更严格的隐私保证。这种方法的主要思想是利用以下事实：用户来自具有一定分布数据的特定人群，因此，他们的更新可能会彼此一致。在许多机器学习场景中，这是一个有意义的假设，因为它们针对的是特定类型的数据（例如医学图像，电子邮件，运动传感器数据等）。例如，尝试将隐藏的音频记录隐藏在ECG分析的训练集中可能是不合理的，因为它出现的可能性实际上比δ小得多。

我们将一种有效而严格的隐私权会计方法（适用于贝叶斯差异性隐私）适用于联邦环境，以便估算客户的隐私权保证。此外，我们强调了实例级隐私的重要性，并提出了此级别的隐私计费的两种变体。最后，我们介绍了一种新的联合计费技术，该技术适用于仅从实例级噪声联合获取实例级和客户端级的隐私保证。

### FedSel: Federated SGD under Local Differential Privacy with Top-k  Dimension Selection

##### 摘要

随着小设备产生大量数据，在移动设备上进行联合学习已成为一种新兴趋势。在联邦环境中，随机梯度下降（SGD）已广泛用于各种机器学习模型的联邦学习中。为了防止根据用户的敏感数据计算出的梯度导致的隐私泄漏，最近将本地差分隐私（LDP）纳入了SGD的隐私保证中。但是，现有的解决方案具有尺寸依赖性问题：注入的噪声与维度d基本成比例。在这篇文章中，提出了一个两阶段的框架——FedSel，在联邦SGD下的LDP中解决这个问题。关键思想是，并非所有维度都同样重要，因此我们根据联邦SGD每次迭代中的贡献私下选择Top-k维度。特别是，我们提出了三种私有维度的选举机制，并采用了渐进式的累积技术来稳定学习过程，而不会产生任何噪音。文章还从理论上分析了FedSel的隐私、准确度和时间复杂度，其性能优于最新的解决方案。在现实世界和综合数据集上的实验证明了我们框架的有效性和效率。

##### 贡献

首先尝试缓和使用LDP的联邦学习的维度依赖问题，我们设计、实现并评价了一个两阶段的$\epsilon-\mathrm{LDP}$框架实现联邦梯度下降。

第三节：提出了一个两阶段的框架FedSel，分别是维度选择和值扰动，在3.2节进行了隐私分析，对于任何客户端的本地向量满足了$\epsilon-$LDP，3.3节的理论分析表明LDP估计误差中的维度依赖从$O(\sqrt{d})$ to $O(1 / \sqrt{d})$。我们还增强了框架，通过累积延迟梯度来避免精度损失。直观上，延迟梯度可以改善经验性能并解决收敛问题。为了进一步在有噪声的私人环境中稳定学习，我们修改了现有的积累[19]。我们的分析和实验证实，这种修改减少了噪声更新的方差。

第四节：使用三种机制实例化了“维度选择”阶段。这三种机制是通用的并且独立于第二阶段的值扰动。提供了选择方案的隐私保证。此外，通过分析和实验将Top-1扩展到Top-k案例，我们展示了实用程序和计算成本的进步。

第五节：最后，我们在合成和真实数据集上进行了广泛的实验，以评估所提出的框架和私有的Top-k维度选择机制。我们还实施超参数自由策略，以在两个阶段之间自动分配隐私预算，从而提高实用性。与现有解决方案相比，测试准确性显示出显着的改进。

![](http://img.wanghaojun.cn//img/20210107161921.png)

![](http://img.wanghaojun.cn//img/20210107162114.png)

##### 总结

文章针对联邦梯度下降算法提出了一个两阶段的LDP私有框架FedSel，其关键思想是首次尝试通过延迟不重要的梯度减轻了注入噪声的维度问题。我们通过在噪声更新时以较小方差修改累积来进一步稳定全局迭代。理论上对所提出方法的改进进行了实验分析和验证。不含超参数的框架在各种批次大小上也优于基线。在未来的工作中，我们计划将实用性和权责发生制的最优折衷形式化为更一般的情况。

### LDP-Fed: Federated Learning with Local Differential Privacy

文章提出了LDP-Fed，一个新颖的联邦学习系统，使用本地差分隐私（LDP）提供正式的隐私保证。开发现有的LDP协议主要是为了确保单个数字或类别值（例如Web访问日志中的点击计数）集合中的数据隐私。但是，在联合学习模型中，参数更新是从每个参与者中反复收集的，并且由高精度的高维连续值（小数点后10位数）组成，这使得现有的LDP协议不适用。为了应对LDP馈送中的这一挑战，我们设计和开发了两种新颖的方法。首先，LDP-Fed的LDP模块提供了正式的差分隐私保证，该模型在多个个体参与者的私有数据集的大规模神经网络的联合训练中重复收集模型训练参数。其次，LDP-Fed实现了一系列选择和过滤技术，用于与参数服务器干扰和共享选择的参数更新。我们验证了使用压缩LDP协议部署的系统在针对公共数据训练深度神经网络时的有效性。在模型准确性，隐私保护和系统功能方面，我们将这种版本的LDP-Fed（称为CLDP-Fed）与其他最新方法进行了比较。

贡献：我们开发了一个联邦学习过程，该过程在复杂的模型参数更新上，根据本地隐私预算，执行基于LDP的扰动，同时最小化噪声对机器学习过程的影响。另外，还提出了参数更新共享的方法，用于在迭代LDP-Fed训练过程的各个回合中选择性共享模型参数更新。我们根据最新的隐私保护FL方法评估LDP-Fed的准确性和系统功能。

![](http://img.wanghaojun.cn//img/20210107201305.png)

模型引入了两个新的组件：在每个客户端运行的本地差分隐私模块，一个k-客户端选择模块。

### Exploring Private Federated Learning with Laplacian Smoothing

联合学习旨在通过协作学习模型而不在用户之间共享私有数据来保护数据隐私。但是，攻击者仍然可以通过攻击已发布的模型来推断私人训练数据。差异性隐私（DP）提供了针对此类攻击的统计保证，其隐私性可能会降低训练模型的准确性或实用性。在本文中，我们将基于拉普拉斯平滑的效用增强方案应用于差分私有联合学习（DP-Fed-LS），其中采用高斯噪声注入的参数聚合在统计精度上得到了改善。我们为均匀二次采样和泊松二次抽样提供紧密的封闭形式隐私边界，并为差分私有的联合学习（有无Laplacian平滑）提供了相应的DP保证。在MNIST，SVHN和Shakespeare数据集上进行的实验表明，在两种二次采样机制下，该方法均可以通过DP保证提高模型的准确性。

在本文中，我们研究了差异化私有联合学习（DP-Fed）并表征了其隐私预算。然后，我们利用拉普拉斯平滑（DP-Fed-LS）来增强训练的效果，同时保持差分隐私。我们作品的主要贡献是：

- 我们为均匀二次抽样和Poisson二次抽样提供了严格的封闭形式隐私边界，从而放宽了先前工作的要求[43,6,29]。根据这些结果，我们得出差分私人联合学习的DP保证，无论有无拉普拉斯平滑。
- 我们通过一系列实验展示了DP-Fed中拉普拉斯平滑的效率，在IID设置下，包括MINST上的逻辑回归模型，在扩展SVHN上的CNN模型；在No-IID设置下，在莎士比亚数据集上的的一个LSTM模型。这些实验表明，在不同的DP保证和二次采样机制下，DP-Fed-LS提供的效用比DP-Fed更好。

DP-Fed-LS过程：

![](http://img.wanghaojun.cn//img/20210108154131.png)

均匀二次采样和泊松二次采样的不同：

![](http://img.wanghaojun.cn//img/20210108155141.png)

### Distributed Differentially Private Averaging with Improved Utility and Robustness to Malicious Parties

与联合学习中一样，从几方拥有的数据中学习，给提供给参与者的隐私保证以及在存在恶意方的情况下计算的正确性提出了挑战。我们在分布式平均的背景下应对这些挑战，分布式平均是分布式和联合学习的基本组成部分。我们的第一个贡献是分发了一个自然地随参与方的数量而扩展的差分私有协议。协议交换基础上的关键思想与边缘sofa网络图上的高斯噪声相关，并由各方添加的独立噪声进行补充。我们分析了协议的差异性隐私保证以及图拓扑的影响，表明即使双方仅与随机选择的其他对数通信，我们也可以匹配可信策展人模型的准确性。这与本地保密模型中的协议（准确性较低）或基于安全聚合（所有用户对都需要交换消息）的协议形成对比。我们的第二个贡献是使用户能够证明他们计算的正确性，而又不影响协议的效率和保密性。我们的构建依赖于标准的加密原语，例如承诺方案和零知识证明。

1 介绍

因此，近年来，具有差分隐私保证的分布式平均吸引了很多兴趣。在本地差分隐私（LDP）的强大模型中[53,30,51,52,49]，每个用户在将其输入发送到不受信任的聚合器之前都会对其本地进行随机化。不幸的是，估计平均用户的最佳误差是O（√n）的因数，它比DP的集中式模型大，后者是受信任的策展人以清晰的方式聚合数据并扰动输出。为了填补这一空白，一些工作探索了LDP的放松，从而有可能匹配可信策展人模型的准确性。这是通过使用诸如密码共享[32]，安全聚合[23,64,14,48]和安全改组[34,25,6]之类的密码原语来实现的。但是，这些原语中的许多原语都假定所有用户真实地遵循该协议（他们诚实但又好奇），并且当参与方数量很大时，它们通常很难处理。

主要有两个贡献：

第一，我们提出了一种新颖的差分私有平均协议，该协议可以匹配可信管理者设置的准确性，同时自然地扩展到大量用户。我们的方法，类似于Gossip协议，被称为GOPA（私有平均Gossip噪声），它是简单且分散的。主要思想是让用户沿着网络边缘（表示为连接图）交换相关的高斯噪声，以便掩盖其私有值而不影响全局平均值。每个用户通过添加独立的（无法消除）高斯噪声来补充（最终消除）这个噪声。我们通过将对手的知识（恶意用户）建模为线性方程组系统来分析GOPA的隐私，并表明隐私保证取决于诚实但好奇的用户子图的生成树的分支因子。值得注意的是，我们建立了只要连接了诚实但好奇的用户图并且成对相关的噪声方差足够大的情况，便可以恢复受信任策展人设置的隐私-效用折衷。最后，为了证明该图的连通性，可以显着减小该方差。最后，为了确保实践中的可伸缩性和鲁棒性，我们提出了一种有效的随机化过程来构建图形，其中每个用户只需要与其他用户的对数通信，同时仍要与隐私-实用性权衡取舍可信策展人模型。我们通过利用和调整随机图论的结果将生成树嵌入随机图来证明这些保证。

我们的第二个贡献是可以使不受信任的外部伙伴验证GOPA的程序，即为用户提供证明其计算正确性而又不损害协议的效率或保密性保证的方法。作为加密货币。这些最初在[43]中正式化的密码原语，在网络通信和计算要求方面都可以很好地扩展，并且非常适合我们不受信任的分散式环境。我们使用经典和最新的ZKP来设计具有可验证分布的噪声生成程序，并最终证明最终计算的完整性（或检测不遵循该协议的恶意用户）。至关重要的是，此程序不会损害协议的保密性（甚至不会放松加密）硬度假设），而计算的完整性则取决于标准离散对数假设。最后，我们的协议提供了正确性保证，基本上与服务提供商自己保留用户的私人数据的情况相同。

第二部分介绍问题设置和对手以及隐私模型。

第三节介绍GOPA协议

第四节分析协议的差分隐私保证

第五节介绍了协议可以面对恶意行为保持正确性

第六节介绍总的计算和通信成本

第七节讨论相关工作

第八节总结和未来方向

2 定义和设置

我们考虑了两个常用的对手模型，它们由[42]进行了形式化，并被用于许多安全协议的设计中。诚实但好奇的用户（简称honest）将遵循协议规范，但可以使用在执行过程中获得的所有信息来推断有关其他用户的信息。相反，恶意用户可能会通过在任何时候发送不正确的值来偏离协议的执行（我们假设它们遵循了所需的通信策略；如果不正确，则很容易检测到）。恶意用户可能会串通在一起，因此将被视为可以访问恶意用户收集的所有信息的单个恶意方（攻击者）（在特殊情况下，可以获得有关单个诚实用户的隐私）。我们的隐私权保证将基于诚实用户通过安全通道进行通信的假设，而我们协议的正确性将通过某种形式的离散对数假设（DLA）（一种密码学的标准假设）得到保证（有关详细信息，请参见附录B）。

3 GOPA

![](http://img.wanghaojun.cn//img/20210110151503.png)

剩下没看

### Federated Learning with Differential Privacy: Algorithms and Performance Analysis

摘要

作为分布式机器学习的一种方式，联合学习（FL）能够显着保护客户的私有数据，使其免受外部对手的攻击。